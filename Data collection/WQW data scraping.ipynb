{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66c919-77ff-4b8a-ad24-c892ab24dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook scrapes data from WQW (https://wqw2010.blogspot.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee96cd0-9d9c-45ab-81d3-a767d3c02647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce40cef-52fe-4d34-a726-6691af98255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downloader:\n",
    "    def __init__(self):\n",
    "        self.s = requests.session()\n",
    "        retries = Retry(total=5,\n",
    "                        backoff_factor=0.1,\n",
    "                        status_forcelist=[ 500, 502, 503, 504 ])\n",
    "        self.s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    \n",
    "    def get(self, keyword, start_num):\n",
    "        resp = self.s.get(\"https://wqw2010.blogspot.com/search\",\n",
    "        params={\n",
    "            \"q\": keyword,\n",
    "            \"max-results\": \"20\",\n",
    "            \"start\": start_num,\n",
    "            \"by-date\": \"true\",\n",
    "        },\n",
    "        headers={\n",
    "            \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "            \"accept-encoding\": \"gzip, deflate, br, zstd\",\n",
    "            \"accept-language\": \"en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7\",\n",
    "            \"priority\": \"u=0, i\",\n",
    "            \"sec-ch-ua\": \"\\\"Google Chrome\\\";v=\\\"129\\\", \\\"Not=A?Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"129\\\"\",\n",
    "            \"sec-ch-ua-mobile\": \"?0\",\n",
    "            \"sec-ch-ua-platform\": \"\\\"macOS\\\"\",\n",
    "            \"sec-fetch-dest\": \"document\",\n",
    "            \"sec-fetch-mode\": \"navigate\",\n",
    "            \"sec-fetch-site\": \"none\",\n",
    "            \"sec-fetch-user\": \"?1\",\n",
    "            \"upgrade-insecure-requests\": \"1\",\n",
    "            \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\",\n",
    "        })\n",
    "        \n",
    "        return resp.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c4d7f5-cb04-4d6c-9e1f-edcb03dff19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def download_one_key(keyword):\n",
    "    d = Downloader()\n",
    "    outdir = Path('../WQW') / keyword\n",
    "    if not outdir.exists():\n",
    "        outdir.mkdir()\n",
    "\n",
    "    for num in range(0, 601, 20):\n",
    "        outf = outdir / f'start={num}.html'\n",
    "        if not outf.exists():\n",
    "            result = d.get(keyword, num)\n",
    "            try:\n",
    "                result = result.decode('utf-8')\n",
    "            except:\n",
    "                print(result)\n",
    "                raise\n",
    "            time.sleep(0.6)\n",
    "            with open(outf, 'w') as f:\n",
    "                f.write(result)\n",
    "                tqdm.write(f'done {outf}')\n",
    "        else:\n",
    "            tqdm.write(f'file {outf} already exists, skipping')\n",
    "            break\n",
    "\n",
    "# try new keywords\n",
    "keywords = ['雇佣黑社会', '带领黑社会', '组织黑社会', '指使黑社会', '勾结黑社会', '安排黑社会', '聘请黑社会', '打手', '小混混', '闲散人员', '地痞流氓']\n",
    "\n",
    "def main():\n",
    "    for k in keywords:\n",
    "        download_one_key(k)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff715399-db80-4a78-a24a-922b134ae501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out ‘没有符合“XX”查询条件的博文‘\n",
    "\n",
    "# Generate the list of file paths based on the output files\n",
    "file_paths = []\n",
    "for k in keywords:\n",
    "    for num in range(0, 601, 20):\n",
    "        file_paths.append(f'../WQW/{k}/start={num}.html')\n",
    "\n",
    "\n",
    "filtered_files = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    if Path(file_path).exists():\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            if \"没有符合\" not in content:\n",
    "                filtered_files.append(file_path)\n",
    "\n",
    "# Print the filtered file paths\n",
    "print(\"Files that do not contain '没有符合':\")\n",
    "for file_path in filtered_files:\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39f751-d217-434c-8e17-e0ab805b17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etract article ID as a list (and remove repetition)\n",
    "\n",
    "import re\n",
    "\n",
    "# Initialize an empty set to store unique tweet IDs\n",
    "all_tweet_ids = set()\n",
    "\n",
    "# Loop through each file path in filtered_files\n",
    "for file_path in filtered_files:\n",
    "    # Open the file and read its content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        result_text = file.read()\n",
    "        \n",
    "        # Apply the regular expression to extract tweet IDs\n",
    "        tweet_ids = re.findall(r\"<a href='https://wqw2010\\.blogspot\\.com/([^']+)\", result_text)\n",
    "\n",
    "        # Filter out tweet_ids that start with 'search/label'\n",
    "        tweet_ids = [tweet_id for tweet_id in tweet_ids if not tweet_id.startswith('search')]\n",
    "        \n",
    "        # Add the tweet IDs found in the current file to the set\n",
    "        all_tweet_ids.update(tweet_ids)\n",
    "\n",
    "# Convert the set back to a list if you need to maintain the order\n",
    "all_tweet_ids = list(all_tweet_ids)\n",
    "\n",
    "# Print all unique extracted tweet IDs\n",
    "print(\"Extracted Tweet IDs:\")\n",
    "for tweet_id in all_tweet_ids:\n",
    "    print(tweet_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d9c1f-2d17-442f-9235-d1bc19b09716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete \"#more\"\n",
    "\n",
    "# Use list comprehension to remove \"#more\" if present\n",
    "cleaned_ids = set([s.split(\"#more\")[0] for s in all_tweet_ids])\n",
    "\n",
    "# Output the cleaned list\n",
    "print(cleaned_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebcc419-ad09-454b-a32b-79c9fee58385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download articles with their links\n",
    "\n",
    "class Downloader_new:\n",
    "    url = \"https://wqw2010.blogspot.com/\"\n",
    "    def __init__(self):\n",
    "        self.s = requests.session()\n",
    "        retries = Retry(total=5,\n",
    "                        backoff_factor=0.1,\n",
    "                        status_forcelist=[ 500, 502, 503, 504 ])\n",
    "        self.s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    \n",
    "    def get(self, tweet_id):\n",
    "        # Correctly format the URL\n",
    "        full_url = f\"{self.url}{tweet_id}\"  \n",
    "\n",
    "        # Make the GET request\n",
    "        resp = self.s.get(full_url,  \n",
    "                          \n",
    "                          headers={\n",
    "            \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "            \"accept-encoding\": \"gzip, deflate, br, zstd\",\n",
    "            \"accept-language\": \"en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7\",\n",
    "            \"cache-control\": \"max-age=0\",\n",
    "            \"if-modified-since\": \"Wed, 16 Oct 2024 09:16:08 GMT\",\n",
    "            \"if-none-match\": \"W/\\\"860da177119340014b0171b62494b980b609ec664d809496c24f4f8fd60ddd36\\\"\",\n",
    "            \"priority\": \"u=0, i\",\n",
    "            \"sec-ch-ua\": \"\\\"Google Chrome\\\";v=\\\"129\\\", \\\"Not=A?Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"129\\\"\",\n",
    "            \"sec-ch-ua-mobile\": \"?0\",\n",
    "            \"sec-ch-ua-platform\": \"\\\"macOS\\\"\",\n",
    "            \"sec-fetch-dest\": \"document\",\n",
    "            \"sec-fetch-mode\": \"navigate\",\n",
    "            \"sec-fetch-site\": \"none\",\n",
    "            \"sec-fetch-user\": \"?1\",\n",
    "            \"upgrade-insecure-requests\": \"1\",\n",
    "            \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\",\n",
    "        })\n",
    "\n",
    "        return resp.content\n",
    "\n",
    "\n",
    "def main():\n",
    "    d = Downloader_new()\n",
    "    for tweet_id in tqdm(cleaned_ids):\n",
    "        outf_new = Path(f'../WQW/all/{tweet_id.replace(\"/\", \"-\")}')\n",
    "        if not outf_new.exists():\n",
    "            result = d.get(tweet_id).decode('utf-8')\n",
    "            time.sleep(0.1)\n",
    "            with open(outf_new, 'w') as f:\n",
    "                f.write(result)\n",
    "        print(outf_new)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9696a-2ad8-4f90-b644-d42a402aee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse information from html using BeautifulSoup\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_blog_info(data):\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    # Extract blog id\n",
    "    blog_id = soup.find('div', class_='post-body entry-content')['id'].split('-')[-1]\n",
    "\n",
    "    # Extract time, title and content\n",
    "    posted_date = soup.find('h2', class_='date-header').text.strip()\n",
    "    title = soup.find('h3', class_='post-title entry-title').text.strip()\n",
    "    content = soup.find('div', class_='post-body entry-content').text.strip()\n",
    "\n",
    "    # Construct the tweet object with the relevant data\n",
    "    tweet = {\n",
    "        \"blogid\": blog_id,  # Blog id\n",
    "        \"posted_date\": posted_date,  # Article posted time\n",
    "        \"title\": title,  # Extracted title\n",
    "        \"content\": content  # Extracted content\n",
    "    }\n",
    "\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd7d7f-8c91-4ff7-86e7-b173714442c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the files\n",
    "directory = \"../WQW/all/\"\n",
    "\n",
    "# Initialize a list to hold all parsed data\n",
    "parsed_data = []\n",
    "\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    # Skip if it's a directory or a checkpoint directory\n",
    "    if os.path.isdir(file_path) or '.ipynb_checkpoints' in filename:\n",
    "        continue\n",
    "        \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        try:\n",
    "            tweet_info = parse_blog_info(content)\n",
    "        except:\n",
    "            print('failed', filename)\n",
    "            import pprint\n",
    "            # pprint.pprint(data)\n",
    "            raise\n",
    "        # Append the parsed data to the list\n",
    "        parsed_data.append(tweet_info)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(parsed_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad41781-a874-46fc-9cac-ea99e5255966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file or do further processing\n",
    "df.to_excel(\"../WQW/all_posts_wqw.xlsx\", index=False)\n",
    "df.to_parquet(\"../WQW/all_posts_wqw.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae86de-c35f-45b0-83ff-3ebae3406ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
