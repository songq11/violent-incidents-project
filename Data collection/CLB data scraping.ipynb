{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd737d7-4390-411e-bcb0-0b3d37576666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook scrapes data from https://clb.org.hk/zh-hans using Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd7b9e-e9ba-41d5-9f1d-952357b258b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dcfb5e-5753-44c3-83db-2ff194afb371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b148630-b303-44bb-b3c7-7e9e8ae93cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Set up the WebDriver (for Chrome)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Perform a Google search \n",
    "def google_search(query):\n",
    "    driver.get(\"https://www.google.com\")\n",
    "    search_box = driver.find_element(\"name\", \"q\")\n",
    "    search_box.send_keys(query)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(3)  # Wait for the results to load\n",
    "\n",
    "# Scrape results from the current page\n",
    "def scrape_results():\n",
    "    results = driver.find_elements(\"css selector\", \"div.g\")  # CSS selector for each result block\n",
    "    links = []\n",
    "    for result in results:\n",
    "        try:\n",
    "            title = result.find_element(\"tag name\", \"h3\").text\n",
    "            link = result.find_element(\"tag name\", \"a\").get_attribute(\"href\")\n",
    "            links.append((title, link))\n",
    "        except NoSuchElementException:\n",
    "            continue  # Skip any incomplete results (e.g., ads or non-standard elements)\n",
    "    return links\n",
    "\n",
    "# Navigate to the next page\n",
    "def go_to_next_page():\n",
    "    try:\n",
    "        next_button = driver.find_element(\"id\", \"pnnext\")  # This is Google's Next button ID\n",
    "        next_button.click()\n",
    "        time.sleep(3)  # Wait for the next page to load\n",
    "        return True\n",
    "    except NoSuchElementException:\n",
    "        return False  # No more pages\n",
    "\n",
    "# Full scraping process for multiple pages\n",
    "def scrape_all_pages(query):\n",
    "    google_search(query)\n",
    "    all_results = []\n",
    "    \n",
    "    while True:\n",
    "        results = scrape_results()\n",
    "        all_results.extend(results)\n",
    "        \n",
    "        if not go_to_next_page():  # Check if there's a next page\n",
    "            break\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Save the results to Excel\n",
    "def save_to_excel(data, filename=\"../CLB/search_results.xlsx\"):\n",
    "    # Convert the list of DataFrames to a single DataFrame\n",
    "    df = pd.concat(data, ignore_index=True)\n",
    "    # Save to Excel\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "# Use multiple keywords\n",
    "keyword_root = ['雇佣黑社会', '带领黑社会', '组织黑社会', '指使黑社会', '勾结黑社会', '安排黑社会', '聘请黑社会', '打手', '小混混', '闲散人员', '地痞流氓']\n",
    "\n",
    "# Prepend 'site:clb.org.hk ' to each keyword\n",
    "keywords = [f\"site:clb.org.hk {keyword}\" for keyword in keyword_root]\n",
    "\n",
    "all_search_results = []\n",
    "\n",
    "for keyword in keywords:\n",
    "    print(f\"Scraping results for: {keyword}\")\n",
    "    search_results = scrape_all_pages(keyword)\n",
    "    \n",
    "    # Add the keyword to each result and convert to a DataFrame\n",
    "    search_results_with_keyword = [(keyword, title, link) for title, link in search_results]\n",
    "    df = pd.DataFrame(search_results_with_keyword, columns=[\"Keyword\", \"Title\", \"URL\"])\n",
    "    \n",
    "    all_search_results.append(df)\n",
    "\n",
    "# Save the combined results for all keywords into an Excel file\n",
    "save_to_excel(all_search_results, \"../CLB/multi_keyword_search_results.xlsx\")\n",
    "\n",
    "# Close the browser when done\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213195c6-f01c-422e-9984-6b8fb8e5f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download webpages with links\n",
    "\n",
    "class Downloader_new:\n",
    "    def __init__(self):\n",
    "        self.s = requests.session()\n",
    "        retries = Retry(total=5,\n",
    "                        backoff_factor=0.1,\n",
    "                        status_forcelist=[ 500, 502, 503, 504 ])\n",
    "        self.s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    \n",
    "    def get(self, url):       \n",
    "        resp = self.s.get(url,  \n",
    "                          headers={\n",
    "                                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "                                \"accept-encoding\": \"gzip, deflate, br, zstd\",\n",
    "                                \"accept-language\": \"en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7\",\n",
    "                                \"cache-control\": \"max-age=0\",\n",
    "                                \"priority\": \"u=0, i\",\n",
    "                                \"sec-ch-ua\": \"\\\"Google Chrome\\\";v=\\\"129\\\", \\\"Not=A?Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"129\\\"\",\n",
    "                                \"sec-ch-ua-mobile\": \"?0\",\n",
    "                                \"sec-ch-ua-platform\": \"\\\"macOS\\\"\",\n",
    "                                \"sec-fetch-dest\": \"document\",\n",
    "                                \"sec-fetch-mode\": \"navigate\",\n",
    "                                \"sec-fetch-site\": \"none\",\n",
    "                                \"sec-fetch-user\": \"?1\",\n",
    "                                \"upgrade-insecure-requests\": \"1\",\n",
    "                                \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\",\n",
    "                            },\n",
    "                            cookies={\n",
    "                                \"_ga\": \"GA1.1.260611918.1729007018\",\n",
    "                                \"gdpr_compliance\": \"agreed\",\n",
    "                                \"_ga_JDSZ292J02\": \"GS1.1.1729525109.3.1.1729526164.60.0.0\",\n",
    "                                \"_ga_LM1DL45R6M\": \"GS1.1.1729625385.8.1.1729625418.27.0.0\",\n",
    "                            })\n",
    "\n",
    "        return resp.content\n",
    "\n",
    "# all_search_results = pd.concat(all_search_results, ignore_index=True)\n",
    "all_search_results = pd.concat(all_search_results)\n",
    "all_url = list(all_search_results['URL'])\n",
    "\n",
    "# Sanitize URL to create a safe filename\n",
    "def sanitize_url(url):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", url)\n",
    "    \n",
    "def main():\n",
    "    d = Downloader_new()\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    output_dir = Path('../CLB/all/')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for url in tqdm(all_url):\n",
    "        safe_filename = sanitize_url(url)[:100]  # Ensure the filename is valid and not too long\n",
    "        outf_new = output_dir / f'{safe_filename}.html'\n",
    "        \n",
    "        if not outf_new.exists():\n",
    "            try:\n",
    "                result = d.get(url).decode('utf-8')\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "                # Write HTML content to file\n",
    "                with open(outf_new, 'w', encoding='utf-8') as f:\n",
    "                    f.write(result)\n",
    "                print(f\"Downloaded: {outf_new}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url}: {e}\")\n",
    "        else:\n",
    "            print(f\"Already exists: {outf_new}\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77def5-687e-4b05-8604-a69126357a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse information from html using BeautifulSoup\n",
    "\n",
    "\n",
    "def parse_blog_info(data):\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    \n",
    "    # Extract URL\n",
    "    link_tag = soup.find('link', rel='alternate')\n",
    "    if link_tag and link_tag.has_attr('href'):\n",
    "        url = link_tag['href']\n",
    "    else:\n",
    "        url = \"URL not found\"\n",
    "    \n",
    "    # Extract posted date\n",
    "    date_div = soup.find('div', class_='author')\n",
    "    if date_div:\n",
    "        date = date_div.get_text(strip=True)\n",
    "    else:\n",
    "        date = \"Date not found\"\n",
    "    \n",
    "    # Extract title\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        title = title_tag.get_text()\n",
    "    else:\n",
    "        title = \"Title not found\"\n",
    "\n",
    "    # Extract article content\n",
    "    content_div = soup.find('div', class_=\"field field--name-body field--type-text-with-summary field--label-hidden field--item\")\n",
    "    if content_div:\n",
    "        content = content_div.get_text(separator=\"\\n\").strip()\n",
    "    else:\n",
    "        content = \"Content not found\"\n",
    "\n",
    "    # Construct the tweet object with the relevant data\n",
    "    tweet = {\n",
    "        \"blogid\": url,  # Blog id\n",
    "        \"posted_date\": date,  # Article posted time\n",
    "        \"title\": title,  # Extracted title\n",
    "        \"content\": content  # Extracted content\n",
    "    }\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786dae9-f2e8-496a-af8b-8a10bbb90263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the files\n",
    "directory = \"../CLB/all/\"\n",
    "\n",
    "# Initialize a list to hold all parsed data\n",
    "parsed_data = []\n",
    "\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    # Skip if it's a directory or a checkpoint directory\n",
    "    if os.path.isdir(file_path) or '.ipynb_checkpoints' in filename:\n",
    "        continue\n",
    "        \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        try:\n",
    "            tweet_info = parse_blog_info(content)\n",
    "        except:\n",
    "            print('failed', filename)\n",
    "            import pprint\n",
    "            # pprint.pprint(data)\n",
    "            raise\n",
    "        # Append the parsed data to the list\n",
    "        parsed_data.append(tweet_info)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(parsed_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b8b30-77d7-4e5c-a544-a66a65bbac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"../CLB/all_posts_clb.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513c24c-9bd9-4352-a73c-ac4b0531419a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
