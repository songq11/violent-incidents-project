{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b556d5d-e25b-4923-8880-3767aa80ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook scrapes data from Weibo (weibo.com)\n",
    "# Part of the program is from https://github.com/dataabc/weibo-crawler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489618c-cbac-4957-9a93-b45495676510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c08cb-052a-460b-bb84-403249184086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downloader:\n",
    "    def __init__(self):\n",
    "        self.s = requests.session()\n",
    "        retries = Retry(total=5,\n",
    "                        backoff_factor=0.1,\n",
    "                        status_forcelist=[ 500, 502, 503, 504 ])\n",
    "        self.s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    \n",
    "    def get(self, keyword, page, start_time, end_time):\n",
    "        timescope = f\"custom:{start_time}:{end_time}\"\n",
    "        resp = self.s.get(\"https://s.weibo.com/weibo\",\n",
    "            params={\n",
    "                \"q\": keyword,\n",
    "                \"typeall\": \"1\",\n",
    "                \"suball\": \"1\",\n",
    "                \"timescope\": timescope,\n",
    "                \"Refer\": \"g\",\n",
    "                \"page\": str(page),\n",
    "            },\n",
    "        headers={\n",
    "            \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "            \"accept-encoding\": \"gzip, deflate, br, zstd\",\n",
    "            \"accept-language\": \"en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7\",\n",
    "            \"cache-control\": \"max-age=0\",\n",
    "            \"priority\": \"u=0, i\",\n",
    "            \"referer\": \"https://passport.weibo.com/\",\n",
    "            \"sec-ch-ua\": \"\\\"Not/A)Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"126\\\", \\\"Google Chrome\\\";v=\\\"126\\\"\",\n",
    "            \"sec-ch-ua-mobile\": \"?0\",\n",
    "            \"sec-ch-ua-platform\": \"\\\"macOS\\\"\",\n",
    "            \"sec-fetch-dest\": \"document\",\n",
    "            \"sec-fetch-mode\": \"navigate\",\n",
    "            \"sec-fetch-site\": \"same-origin\",\n",
    "            \"sec-fetch-user\": \"?1\",\n",
    "            \"upgrade-insecure-requests\": \"1\",\n",
    "            \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
    "        },\n",
    "        cookies={\n",
    "            \"_s_tentry\": \"-\",\n",
    "            \"Apache\": \"2702705434452.588.1723222797133\",\n",
    "            \"SINAGLOBAL\": \"2702705434452.588.1723222797133\",\n",
    "            \"ULV\": \"1723222797169:1:1:1:2702705434452.588.1723222797133:\",\n",
    "            \"SCF\": \"AkJEZvqK2YnhrqOs5VDcuiy9TaRKPLvDuKc2mdSr12a23ejc9uz1-ytkJXLfSLuDypwbPPWNP-ydzfpD5Y2scZE.\",\n",
    "            \"ALF\": \"1726763082\",\n",
    "            \"SUB\": \"_2A25LwLMaDeRhGeRI4lET8ijLyT6IHXVovErSrDV8PUJbkNANLW3ukW1NUs4-fWVTYDCfj3yakyQhcqTzWdERcq4t\",\n",
    "            \"SUBP\": \"0033WrSXqPxfM725Ws9jqgMF55529P9D9W5RiTdkdGGuk7WzMs_fbBje5JpX5KzhUgL.Fozc1KeEeoqNeoz2dJLoIpXLxKBLB.zL1h-LxKBLB.zL1h-LxKqL1KnLB-93M7tt\",\n",
    "        })\n",
    "        return resp.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7e3b6-fde4-41b0-b9f6-4d9bbb049e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def parse_weibo_time_list(begin_time:str, end_time: str, day_interval: int = 10):\n",
    "    \"\"\"获取 begin 和end的列表集合\n",
    "\n",
    "    Args:\n",
    "        begin_time (str): 开始时间\n",
    "        end_time (str): 结束时间\n",
    "        day_interval (int, optional): 默认间隔时间10天. Defaults to 10.\n",
    "    \"\"\"\n",
    "    time_list = []\n",
    "    begin_time_date_ = datetime.datetime.strptime(begin_time,\"%Y-%m-%d-%H\")\n",
    "    end_time_date_ = datetime.datetime.strptime(end_time,\"%Y-%m-%d-%H\")\n",
    "    if (begin_time_date_+ datetime.timedelta(days=day_interval)) > end_time_date_:\n",
    "        time_ = [begin_time_date_, end_time_date_]\n",
    "        time_list.append(time_)\n",
    "    else:\n",
    "        time_begin_inter = begin_time_date_\n",
    "        end_time_inter = begin_time_date_ +  datetime.timedelta(days=day_interval)\n",
    "        while end_time_inter <= end_time_date_:\n",
    "            time_list.append([time_begin_inter, end_time_inter])\n",
    "            time_begin_inter = time_begin_inter + datetime.timedelta(days=day_interval)\n",
    "            end_time_inter = time_begin_inter + datetime.timedelta(days=day_interval) \n",
    "    for i in time_list:\n",
    "        i[0] = i[0].strftime('%Y-%m-%d-%H')\n",
    "        i[1] = i[1].strftime('%Y-%m-%d-%H')\n",
    "    return time_list\n",
    "\n",
    "\n",
    "search_time = parse_weibo_time_list('2009-08-16-0', '2024-08-20-0', 5)\n",
    "\n",
    "print(search_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10694683-54ca-4326-8c3f-c7984a4a05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def download_one_key(keyword):\n",
    "    d = Downloader()\n",
    "    outdir = Path('output') / keyword\n",
    "    if not outdir.exists():\n",
    "        outdir.mkdir()\n",
    "        \n",
    "    for begin, end in tqdm(search_time):\n",
    "        for page_num in range(1,51):            \n",
    "            outf = outdir / f'{begin}_{end}_{page_num}.html'\n",
    "            if not outf.exists():\n",
    "                result = d.get(keyword, page_num, begin, end)\n",
    "                try:\n",
    "                    result = result.decode('utf-8')\n",
    "                except:\n",
    "                    print(result)\n",
    "                    raise\n",
    "                time.sleep(0.6)\n",
    "                with open(outf, 'w') as f:\n",
    "                    f.write(result)\n",
    "                if \"card-no-result\" in result:\n",
    "                    tqdm.write(f'reached no result {outf}')\n",
    "                    break\n",
    "                else:\n",
    "                    tqdm.write(f'done {outf}')\n",
    "            else:\n",
    "                if \"card-no-result\" in outf.read_text():\n",
    "                    tqdm.write(f'skip {outf}')\n",
    "                    break\n",
    "\n",
    "def main():\n",
    "    for k in ['雇佣黑社会', '雇打手','雇小混混','雇社会闲散人员']:\n",
    "        download_one_key(k)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f041f5-c074-4b60-a799-da5439b7e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out ‘抱歉，未找到相关结果‘\n",
    "\n",
    "# Generate the list of file paths based on the output files\n",
    "file_paths = []\n",
    "for k in ['雇佣黑社会','雇打手','雇小混混','雇社会闲散人员']:\n",
    "    for begin, end in search_time:\n",
    "            for page_num in range(1,51):            \n",
    "                file_paths.append(f'output/{k}/{begin}_{end}_{page_num}.html')\n",
    "\n",
    "\n",
    "filtered_files = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    if Path(file_path).exists():\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            if \"card-no-result\" not in content:\n",
    "                filtered_files.append(file_path)\n",
    "\n",
    "# Print the filtered file paths\n",
    "print(\"Files that do not contain 'card-no-result':\")\n",
    "for file_path in filtered_files:\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f5532-ea52-46b3-8b19-6bbd1540abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etract Weibo ID as a list (and remove repetition)\n",
    "\n",
    "import re\n",
    "\n",
    "# Initialize an empty set to store unique tweet IDs\n",
    "all_tweet_ids = set()\n",
    "\n",
    "# Loop through each file path in filtered_files\n",
    "for file_path in filtered_files:\n",
    "    # Open the file and read its content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        result_text = file.read()\n",
    "        \n",
    "        # Apply the regular expression to extract tweet IDs\n",
    "        tweet_ids = re.findall(r'\\d+/(.*?)\\?refer_flag=1001030103_\\'\\)\">复制微博地址</a>', result_text)\n",
    "        \n",
    "        # Add the tweet IDs found in the current file to the set\n",
    "        all_tweet_ids.update(tweet_ids)\n",
    "\n",
    "# Convert the set back to a list if you need to maintain the order\n",
    "all_tweet_ids = list(all_tweet_ids)\n",
    "\n",
    "# Print all unique extracted tweet IDs\n",
    "print(\"Extracted Tweet IDs:\")\n",
    "for tweet_id in all_tweet_ids:\n",
    "    print(tweet_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6569d93e-8f0d-4670-a601-d45b0c4bd3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download without logging in\n",
    "\n",
    "class Downloader_new:\n",
    "    url = \"https://weibo.com/ajax/statuses/show\"\n",
    "    def __init__(self):\n",
    "        self.s = requests.session()\n",
    "        retries = Retry(total=5,\n",
    "                        backoff_factor=0.1,\n",
    "                        status_forcelist=[ 500, 502, 503, 504 ])\n",
    "        self.s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    \n",
    "    def get(self, tweet_id):\n",
    "        resp = self.s.get(self.url,\n",
    "            params={\n",
    "                \"id\": tweet_id,\n",
    "            },\n",
    "            headers={\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "                \"accept-encoding\": \"gzip, deflate, br, zstd\",\n",
    "                \"accept-language\": \"en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7\",\n",
    "                \"cache-control\": \"max-age=0\",\n",
    "                \"priority\": \"u=0, i\",\n",
    "                \"sec-ch-ua\": \"\\\"Not/A)Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"126\\\", \\\"Google Chrome\\\";v=\\\"126\\\"\",\n",
    "                \"sec-ch-ua-mobile\": \"?0\",\n",
    "                \"sec-ch-ua-platform\": \"\\\"macOS\\\"\",\n",
    "                \"sec-fetch-dest\": \"document\",\n",
    "                \"sec-fetch-mode\": \"navigate\",\n",
    "                \"sec-fetch-site\": \"none\",\n",
    "                \"sec-fetch-user\": \"?1\",\n",
    "                \"upgrade-insecure-requests\": \"1\",\n",
    "                \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
    "            },\n",
    "            cookies={\n",
    "                \"XSRF-TOKEN\": \"bRT86qipdndDABwvrUrtgLHu\",\n",
    "                \"SUB\": \"_2AkMR6sQzdcPxrAFXkfsXzm3lbo9H-jyiP63FAn7uJhMyOhgP7nM1qSdutBF-XAXYLzAXdMZbjazkR8IdeVv_eg7b\",\n",
    "                \"SUBP\": \"0033WrSXqPxfM72wWs9jqgMF55529P9D9W5RiTdkdGGuk7WzMs_fbBje5JpVhgU4PcDuBh2ESb4odcXt\",\n",
    "                \"_s_tentry\": \"-\",\n",
    "                \"Apache\": \"2702705434452.588.1723222797133\",\n",
    "                \"SINAGLOBAL\": \"2702705434452.588.1723222797133\",\n",
    "                \"ULV\": \"1723222797169:1:1:1:2702705434452.588.1723222797133:\",\n",
    "            })\n",
    "\n",
    "        return resp.content\n",
    "\n",
    "\n",
    "def main():\n",
    "    d = Downloader_new()\n",
    "    for tweet_id in tqdm(all_tweet_ids):\n",
    "        outf_new = Path(f'clean/{tweet_id}')\n",
    "        if not outf_new.exists():\n",
    "            result = d.get(tweet_id).decode('utf-8')\n",
    "            time.sleep(0.1)\n",
    "            with open(outf_new, 'w') as f:\n",
    "                f.write(result)\n",
    "        print(outf_new)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448dff6-739b-45b3-9fde-1065dbc0b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil\n",
    "def parse_time(s):\n",
    "    \"\"\"\n",
    "    Wed Oct 19 23:44:36 +0800 2022 => 2022-10-19 23:44:36\n",
    "    \"\"\"\n",
    "    # return \"2022-10-19 23:44:36\"\n",
    "    return dateutil.parser.parse(s).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "def parse_user_info(data):\n",
    "    \"\"\"\n",
    "    解析用户信息\n",
    "    \"\"\"\n",
    "    # 基础信息\n",
    "    user = {\n",
    "        \"_id\": str(data['id']),\n",
    "        \"nick_name\": data['screen_name'],\n",
    "        \"verified\": data['verified'],\n",
    "    }\n",
    "    return user\n",
    "\n",
    "def parse_blog_info(data):\n",
    "        # print(f\"blog_data:{data}\")\n",
    "        user = parse_user_info(data['user'])\n",
    "        tweet = {\n",
    "            \"_id\": str(data['mid']),\n",
    "            \"mblogid\": data['mblogid'],  # 博客id\n",
    "            \"created_at\": parse_time(data['created_at']),  # 文章发布时间\n",
    "            \"user_id\": user['_id'],\n",
    "            'user_nickname': user['nick_name'],\n",
    "            'user_verified': user['verified'],\n",
    "            \"content\": data.get('text_raw',g_none_word).replace('\\u200b', ''),\n",
    "            'content_long':'',\n",
    "            \"geo\": data.get('geo',g_none_word),\n",
    "            \"ip_location\": data.get('region_name', g_none_word),\n",
    "            \"reposts_count\": data.get('reposts_count',g_none_word),\n",
    "            \"comments_count\": data.get('comments_count',g_none_word),\n",
    "            \"attitudes_count\": data.get('attitudes_count',g_none_word),\n",
    "            \"source\": data.get(\"source\",g_none_word),\n",
    "            \"pic_urls\": [\"https://wx1.sinaimg.cn/orj960/\" + pic_id for pic_id in data.get('pic_ids', [])],\n",
    "            \"pic_num\": data['pic_num'],\n",
    "            'is_long_text': 'continue_tag' in data and data['isLongText'],\n",
    "            \"video\": 'N/A'\n",
    "        }\n",
    "        if 'page_info' in data and data['page_info'].get('object_type', '') == 'video':\n",
    "            med_info = data['page_info'].get('media_info')\n",
    "            if med_info is None:\n",
    "                tweet['video'] = 'N/A'\n",
    "            else:\n",
    "                tweet['video'] = med_info['mp4_720p_mp4']\n",
    "        tweet[\"url\"] = f\"https://weibo.com/{user['_id']}/{tweet['mblogid']}\"  # 文章地址\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086bde6-051e-4516-9b9d-5936474cb066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the directory containing the files\n",
    "directory = \"clean\"\n",
    "\n",
    "# Initialize a list to hold all parsed data\n",
    "parsed_data = []\n",
    "\n",
    "g_none_word = 'NA'\n",
    "\n",
    "class DownloaderLongTxt(Downloader_new):\n",
    "    url = \"https://weibo.com/ajax/statuses/longtext\"\n",
    "\n",
    "downloader_long = DownloaderLongTxt()\n",
    "\n",
    "def get_long_text(blog_id):\n",
    "    dir_long = Path(directory) / 'long'\n",
    "    if not dir_long.exists():\n",
    "        dir_long.mkdir()\n",
    "    file_path = dir_long / f'{blog_id}.json'\n",
    "    if not file_path.exists():\n",
    "        print('requesting long text for', blog_id)\n",
    "        file_path.write_bytes(downloader_long.get(blog_id))\n",
    "\n",
    "    with file_path.open('r') as fin:\n",
    "        content = fin.read()\n",
    "        if content == '<h2>400 Bad Request</h2>':\n",
    "            print(\"long 400\", blog_id)\n",
    "            return \"N/A due to 400\"\n",
    "        try:\n",
    "            data = json.loads(content)\n",
    "        except:\n",
    "            print('bad json', file_path)\n",
    "            print(content)\n",
    "            raise\n",
    "    if not data['ok']:\n",
    "        if data['error_code']:\n",
    "            return f'N/A due to {data[\"message\"]}'\n",
    "        return 'N/A due to not OK but empty error code'\n",
    "    if not len(data['data']):\n",
    "        return 'N/A due to empty data'\n",
    "    return data['data']['longTextContent']\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    # Skip if it's a directory or a checkpoint directory\n",
    "    if os.path.isdir(file_path) or '.ipynb_checkpoints' in filename:\n",
    "        continue\n",
    "        \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        if content == '<h2>400 Bad Request</h2>':\n",
    "            print('400', filename)\n",
    "            continue\n",
    "        # Parse the blog information\n",
    "        data = None\n",
    "        try:\n",
    "            data = json.loads(content)\n",
    "            if data.get('error_code', 0) == 20101:\n",
    "                print('missing', filename)\n",
    "                continue\n",
    "            tweet_info = parse_blog_info(data)\n",
    "            if tweet_info['is_long_text']:\n",
    "                tweet_info['content_long'] = get_long_text(tweet_info['mblogid'])\n",
    "            else:\n",
    "                tweet_info['content_long'] = tweet_info['content']\n",
    "        except:\n",
    "            print('failed', filename)\n",
    "            import pprint\n",
    "            # pprint.pprint(data)\n",
    "            raise\n",
    "        # Append the parsed data to the list\n",
    "        parsed_data.append(tweet_info)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(parsed_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file or do further processing\n",
    "df.to_excel(\"all_tweets.xlsx\", index=False)\n",
    "df.to_parquet(\"all_tweets.parquet\", index=False)\n",
    "\n",
    "print(\"DataFrame created and saved to all_tweets.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a437c85-ce77-43be-8976-1c9ff0c63f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c370dba0-0072-4ed9-8252-ee0ac76d87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"all_tweets.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f34818-223c-416f-9c2b-9f7e3857be9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
